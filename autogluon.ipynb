{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BSH/miniconda3/envs/autogluon/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import autogluon\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import bisect\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\"data/train.parquet\").drop(columns=['SAMPLE_ID'])\n",
    "test_df = pd.read_parquet(\"data/test.parquet\").drop(columns=['SAMPLE_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['ATA'] = pd.to_datetime(train_df['ATA'])\n",
    "test_df['ATA'] = pd.to_datetime(test_df['ATA'])\n",
    "\n",
    "# datetime을 여러 파생 변수로 변환\n",
    "for df in [train_df, test_df]:\n",
    "    df['year'] = df['ATA'].dt.year\n",
    "    df['month'] = df['ATA'].dt.month\n",
    "    df['day'] = df['ATA'].dt.day\n",
    "    df['hour'] = df['ATA'].dt.hour\n",
    "    df['minute'] = df['ATA'].dt.minute\n",
    "    df['weekday'] = df['ATA'].dt.weekday\n",
    "\n",
    "# datetime 컬럼 제거\n",
    "train_df.drop(columns='ATA', inplace=True)\n",
    "test_df.drop(columns='ATA', inplace=True)\n",
    "\n",
    "# Categorical 컬럼 인코딩\n",
    "categorical_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG']\n",
    "encoders = {}\n",
    "\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train_df[feature] = le.fit_transform(train_df[feature].astype(str))\n",
    "    le_classes_set = set(le.classes_)\n",
    "    test_df[feature] = test_df[feature].map(lambda s: '-1' if s not in le_classes_set else s)\n",
    "    le_classes = le.classes_.tolist()\n",
    "    bisect.insort_left(le_classes, '-1')\n",
    "    le.classes_ = np.array(le_classes)\n",
    "    test_df[feature] = le.transform(test_df[feature].astype(str))\n",
    "    encoders[feature] = le\n",
    "# 결측치 처리\n",
    "train_df.fillna(train_df.mean(), inplace=True)\n",
    "test_df.fillna(train_df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset(train_df)\n",
    "test_data = TabularDataset(test_df)\n",
    "\n",
    "label = \"CI_HOUR\"\n",
    "eval_metric = \"mean_absolute_error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230926_003757/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (367441 samples, 91.13 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230926_003757/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.2.0: Fri Nov 11 02:04:44 PST 2022; root:xnu-8792.61.2~4/RELEASE_ARM64_T8103\n",
      "Disk Space Avail:   28.93 GB / 245.11 GB (11.8%)\n",
      "Train Data Rows:    367441\n",
      "Train Data Columns: 30\n",
      "Label Column: CI_HOUR\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2127.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 88.19 MB (4.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 14 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
      "\t\t('int', [])   : 16 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 14 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
      "\t\t('int', [])   : 16 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t30 features in original data used to generate 30 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 88.19 MB (4.6% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 0.159 GB out of 1.776 GB available memory (44.682%)... (20.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=0.50 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\tNot enough memory to train KNeighborsUnif_BAG_L1... Skipping this model.\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 0.159 GB out of 1.784 GB available memory (44.498%)... (20.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=0.49 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\tNot enough memory to train KNeighborsDist_BAG_L1... Skipping this model.\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 53.7464\n",
      "[2000]\tvalid_set's l1: 51.1683\n",
      "[3000]\tvalid_set's l1: 49.4552\n",
      "[4000]\tvalid_set's l1: 48.124\n",
      "[5000]\tvalid_set's l1: 47.0637\n",
      "[6000]\tvalid_set's l1: 46.2787\n",
      "[7000]\tvalid_set's l1: 45.586\n",
      "[8000]\tvalid_set's l1: 44.9676\n",
      "[9000]\tvalid_set's l1: 44.4582\n",
      "[10000]\tvalid_set's l1: 44.0386\n",
      "[1000]\tvalid_set's l1: 53.1819\n",
      "[2000]\tvalid_set's l1: 51.0168\n",
      "[3000]\tvalid_set's l1: 49.1289\n",
      "[4000]\tvalid_set's l1: 47.8111\n",
      "[5000]\tvalid_set's l1: 46.7126\n",
      "[6000]\tvalid_set's l1: 45.896\n",
      "[7000]\tvalid_set's l1: 45.2795\n",
      "[8000]\tvalid_set's l1: 44.7178\n",
      "[9000]\tvalid_set's l1: 44.2266\n",
      "[10000]\tvalid_set's l1: 43.703\n",
      "[1000]\tvalid_set's l1: 54.0638\n",
      "[2000]\tvalid_set's l1: 51.4087\n",
      "[3000]\tvalid_set's l1: 49.588\n",
      "[4000]\tvalid_set's l1: 48.0526\n",
      "[5000]\tvalid_set's l1: 46.8886\n",
      "[6000]\tvalid_set's l1: 46.0192\n",
      "[7000]\tvalid_set's l1: 45.2891\n",
      "[8000]\tvalid_set's l1: 44.6527\n",
      "[9000]\tvalid_set's l1: 44.1453\n",
      "[10000]\tvalid_set's l1: 43.7178\n",
      "[1000]\tvalid_set's l1: 54.1301\n",
      "[2000]\tvalid_set's l1: 51.55\n",
      "[3000]\tvalid_set's l1: 49.8748\n",
      "[4000]\tvalid_set's l1: 48.5356\n",
      "[5000]\tvalid_set's l1: 47.4177\n",
      "[6000]\tvalid_set's l1: 46.5637\n",
      "[7000]\tvalid_set's l1: 45.9096\n",
      "[8000]\tvalid_set's l1: 45.2117\n",
      "[9000]\tvalid_set's l1: 44.6487\n",
      "[10000]\tvalid_set's l1: 44.1243\n",
      "[1000]\tvalid_set's l1: 52.7746\n",
      "[2000]\tvalid_set's l1: 50.5006\n",
      "[3000]\tvalid_set's l1: 48.7353\n",
      "[4000]\tvalid_set's l1: 47.3459\n",
      "[5000]\tvalid_set's l1: 46.3306\n",
      "[6000]\tvalid_set's l1: 45.4028\n",
      "[7000]\tvalid_set's l1: 44.692\n",
      "[8000]\tvalid_set's l1: 44.0653\n",
      "[9000]\tvalid_set's l1: 43.5599\n",
      "[10000]\tvalid_set's l1: 43.1875\n",
      "[1000]\tvalid_set's l1: 53.2451\n",
      "[2000]\tvalid_set's l1: 50.6955\n",
      "[3000]\tvalid_set's l1: 49.0879\n",
      "[4000]\tvalid_set's l1: 47.8091\n",
      "[5000]\tvalid_set's l1: 46.7473\n",
      "[6000]\tvalid_set's l1: 45.9338\n",
      "[7000]\tvalid_set's l1: 45.2824\n",
      "[8000]\tvalid_set's l1: 44.7202\n",
      "[9000]\tvalid_set's l1: 44.2184\n",
      "[10000]\tvalid_set's l1: 43.7679\n",
      "[1000]\tvalid_set's l1: 53.7436\n",
      "[2000]\tvalid_set's l1: 51.3184\n",
      "[3000]\tvalid_set's l1: 49.5137\n",
      "[4000]\tvalid_set's l1: 48.2062\n",
      "[5000]\tvalid_set's l1: 47.1869\n",
      "[6000]\tvalid_set's l1: 46.4371\n",
      "[7000]\tvalid_set's l1: 45.766\n",
      "[8000]\tvalid_set's l1: 45.0719\n",
      "[9000]\tvalid_set's l1: 44.5875\n",
      "[10000]\tvalid_set's l1: 44.11\n",
      "[1000]\tvalid_set's l1: 52.8807\n",
      "[2000]\tvalid_set's l1: 50.8315\n",
      "[3000]\tvalid_set's l1: 49.3756\n",
      "[4000]\tvalid_set's l1: 48.1832\n",
      "[5000]\tvalid_set's l1: 47.2109\n",
      "[6000]\tvalid_set's l1: 46.3363\n",
      "[7000]\tvalid_set's l1: 45.5614\n",
      "[8000]\tvalid_set's l1: 44.947\n",
      "[9000]\tvalid_set's l1: 44.4569\n",
      "[10000]\tvalid_set's l1: 43.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-43.8289\t = Validation score   (-mean_absolute_error)\n",
      "\t575.4s\t = Training   runtime\n",
      "\t55.5s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 46.2716\n",
      "[2000]\tvalid_set's l1: 40.2931\n",
      "[3000]\tvalid_set's l1: 36.3474\n",
      "[4000]\tvalid_set's l1: 34.0801\n",
      "[5000]\tvalid_set's l1: 32.2888\n",
      "[6000]\tvalid_set's l1: 31.0332\n",
      "[7000]\tvalid_set's l1: 30.0751\n",
      "[8000]\tvalid_set's l1: 29.1074\n",
      "[9000]\tvalid_set's l1: 28.3619\n",
      "[10000]\tvalid_set's l1: 27.8328\n",
      "[1000]\tvalid_set's l1: 44.6309\n",
      "[2000]\tvalid_set's l1: 39.585\n",
      "[3000]\tvalid_set's l1: 35.9446\n",
      "[4000]\tvalid_set's l1: 33.7409\n",
      "[5000]\tvalid_set's l1: 32.3143\n",
      "[6000]\tvalid_set's l1: 30.898\n",
      "[7000]\tvalid_set's l1: 30.0355\n",
      "[8000]\tvalid_set's l1: 29.2411\n",
      "[9000]\tvalid_set's l1: 28.673\n",
      "[10000]\tvalid_set's l1: 28.0967\n",
      "[1000]\tvalid_set's l1: 45.4007\n",
      "[2000]\tvalid_set's l1: 40.1501\n",
      "[3000]\tvalid_set's l1: 36.4074\n",
      "[4000]\tvalid_set's l1: 33.9071\n",
      "[5000]\tvalid_set's l1: 32.455\n",
      "[6000]\tvalid_set's l1: 31.2144\n",
      "[7000]\tvalid_set's l1: 30.2918\n",
      "[8000]\tvalid_set's l1: 29.5533\n",
      "[9000]\tvalid_set's l1: 28.9883\n",
      "[10000]\tvalid_set's l1: 28.4841\n",
      "[1000]\tvalid_set's l1: 46.1463\n",
      "[2000]\tvalid_set's l1: 39.9051\n",
      "[3000]\tvalid_set's l1: 36.0507\n",
      "[4000]\tvalid_set's l1: 33.9306\n",
      "[5000]\tvalid_set's l1: 32.4707\n",
      "[6000]\tvalid_set's l1: 31.3879\n",
      "[7000]\tvalid_set's l1: 30.6383\n",
      "[8000]\tvalid_set's l1: 29.9563\n",
      "[9000]\tvalid_set's l1: 29.2841\n",
      "[10000]\tvalid_set's l1: 28.6907\n",
      "[1000]\tvalid_set's l1: 44.9172\n",
      "[2000]\tvalid_set's l1: 39.2927\n",
      "[3000]\tvalid_set's l1: 36.0586\n",
      "[4000]\tvalid_set's l1: 33.9954\n",
      "[5000]\tvalid_set's l1: 32.2676\n",
      "[6000]\tvalid_set's l1: 31.0351\n",
      "[7000]\tvalid_set's l1: 30.0227\n",
      "[8000]\tvalid_set's l1: 29.1852\n",
      "[9000]\tvalid_set's l1: 28.5408\n",
      "[10000]\tvalid_set's l1: 27.9667\n",
      "[1000]\tvalid_set's l1: 45.5347\n",
      "[2000]\tvalid_set's l1: 39.5941\n",
      "[3000]\tvalid_set's l1: 35.831\n",
      "[4000]\tvalid_set's l1: 33.473\n",
      "[5000]\tvalid_set's l1: 31.9955\n",
      "[6000]\tvalid_set's l1: 30.7085\n",
      "[7000]\tvalid_set's l1: 29.7896\n",
      "[8000]\tvalid_set's l1: 29.1394\n",
      "[9000]\tvalid_set's l1: 28.5438\n",
      "[10000]\tvalid_set's l1: 27.912\n",
      "[1000]\tvalid_set's l1: 45.0939\n",
      "[2000]\tvalid_set's l1: 39.6554\n",
      "[3000]\tvalid_set's l1: 35.9519\n",
      "[4000]\tvalid_set's l1: 33.7869\n",
      "[5000]\tvalid_set's l1: 32.2955\n",
      "[6000]\tvalid_set's l1: 31.0736\n",
      "[7000]\tvalid_set's l1: 30.215\n",
      "[8000]\tvalid_set's l1: 29.3383\n",
      "[9000]\tvalid_set's l1: 28.6176\n",
      "[10000]\tvalid_set's l1: 28.0325\n",
      "[1000]\tvalid_set's l1: 44.8185\n",
      "[2000]\tvalid_set's l1: 38.8454\n",
      "[3000]\tvalid_set's l1: 35.392\n",
      "[4000]\tvalid_set's l1: 33.4758\n",
      "[5000]\tvalid_set's l1: 31.9487\n",
      "[6000]\tvalid_set's l1: 30.7676\n",
      "[7000]\tvalid_set's l1: 29.738\n",
      "[8000]\tvalid_set's l1: 29.0163\n",
      "[9000]\tvalid_set's l1: 28.3412\n",
      "[10000]\tvalid_set's l1: 27.8231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-28.1047\t = Validation score   (-mean_absolute_error)\n",
      "\t568.36s\t = Training   runtime\n",
      "\t56.79s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 173 due to low memory. Expected memory usage reduced from 25.93% -> 15.0% of available memory...\n",
      "\t-53.5117\t = Validation score   (-mean_absolute_error)\n",
      "\t161.84s\t = Training   runtime\n",
      "\t4.43s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/BSH/Desktop/Project/dacon/HD_challenge/autogluon.ipynb 셀 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/BSH/Desktop/Project/dacon/HD_challenge/autogluon.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predictor \u001b[39m=\u001b[39m TabularPredictor(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/BSH/Desktop/Project/dacon/HD_challenge/autogluon.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     label\u001b[39m=\u001b[39;49mlabel, problem_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mregression\u001b[39;49m\u001b[39m'\u001b[39;49m, eval_metric\u001b[39m=\u001b[39;49meval_metric\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/BSH/Desktop/Project/dacon/HD_challenge/autogluon.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\u001b[39m.\u001b[39;49mfit(train_data, presets\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbest_quality\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/utils/decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     gargs, gkwargs \u001b[39m=\u001b[39m g(\u001b[39m*\u001b[39mother_args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49mgargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:986\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m     aux_kwargs[\u001b[39m\"\u001b[39m\u001b[39mfit_weighted_ensemble\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learner\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    987\u001b[0m     X\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[1;32m    988\u001b[0m     X_val\u001b[39m=\u001b[39;49mtuning_data,\n\u001b[1;32m    989\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49munlabeled_data,\n\u001b[1;32m    990\u001b[0m     holdout_frac\u001b[39m=\u001b[39;49mholdout_frac,\n\u001b[1;32m    991\u001b[0m     num_bag_folds\u001b[39m=\u001b[39;49mnum_bag_folds,\n\u001b[1;32m    992\u001b[0m     num_bag_sets\u001b[39m=\u001b[39;49mnum_bag_sets,\n\u001b[1;32m    993\u001b[0m     num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[1;32m    994\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[1;32m    995\u001b[0m     core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[1;32m    996\u001b[0m     aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[1;32m    997\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[1;32m    998\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    999\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m   1000\u001b[0m     verbosity\u001b[39m=\u001b[39;49mverbosity,\n\u001b[1;32m   1001\u001b[0m     use_bag_holdout\u001b[39m=\u001b[39;49muse_bag_holdout,\n\u001b[1;32m   1002\u001b[0m )\n\u001b[1;32m   1003\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_post_fit_vars()\n\u001b[1;32m   1005\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_fit(\n\u001b[1;32m   1006\u001b[0m     keep_only_best\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mkeep_only_best\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1007\u001b[0m     refit_full\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mrefit_full\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     infer_limit\u001b[39m=\u001b[39minfer_limit,\n\u001b[1;32m   1013\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/tabular/learner/abstract_learner.py:159\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLearner is already fit.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_input(X\u001b[39m=\u001b[39mX, X_val\u001b[39m=\u001b[39mX_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 159\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X\u001b[39m=\u001b[39;49mX, X_val\u001b[39m=\u001b[39;49mX_val, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:157\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_metric \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39meval_metric\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[0;32m--> 157\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    158\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    159\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    160\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    161\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    162\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    163\u001b[0m     holdout_frac\u001b[39m=\u001b[39;49mholdout_frac,\n\u001b[1;32m    164\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit_trainer,\n\u001b[1;32m    165\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    166\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    167\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    168\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrainer_fit_kwargs,\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_trainer(trainer\u001b[39m=\u001b[39mtrainer)\n\u001b[1;32m    171\u001b[0m time_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/tabular/trainer/auto_trainer.py:114\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m log_str \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m logger\u001b[39m.\u001b[39mlog(\u001b[39m20\u001b[39m, log_str)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_and_ensemble(\n\u001b[1;32m    115\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    116\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    117\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    118\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    119\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    120\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[1;32m    121\u001b[0m     num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[1;32m    122\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[1;32m    123\u001b[0m     core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[1;32m    124\u001b[0m     aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[1;32m    125\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    126\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    127\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2371\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[0;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[1;32m   2369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_rows_val \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_val)\n\u001b[1;32m   2370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_cols_train \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(X\u001b[39m.\u001b[39mcolumns))\n\u001b[0;32m-> 2371\u001b[0m model_names_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_multi_levels(\n\u001b[1;32m   2372\u001b[0m     X,\n\u001b[1;32m   2373\u001b[0m     y,\n\u001b[1;32m   2374\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[1;32m   2375\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m   2376\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m   2377\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m   2378\u001b[0m     level_start\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2379\u001b[0m     level_end\u001b[39m=\u001b[39;49mnum_stack_levels \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2380\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[1;32m   2381\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2382\u001b[0m )\n\u001b[1;32m   2383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_model_names()) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2384\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAutoGluon did not successfully train any models\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:395\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    393\u001b[0m         core_kwargs_level[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m core_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m, time_limit_core)\n\u001b[1;32m    394\u001b[0m         aux_kwargs_level[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m, time_limit_aux)\n\u001b[0;32m--> 395\u001b[0m     base_model_names, aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_new_level(\n\u001b[1;32m    396\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    397\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    398\u001b[0m         X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    399\u001b[0m         y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    400\u001b[0m         X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    401\u001b[0m         models\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[1;32m    402\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m    403\u001b[0m         base_model_names\u001b[39m=\u001b[39;49mbase_model_names,\n\u001b[1;32m    404\u001b[0m         core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs_level,\n\u001b[1;32m    405\u001b[0m         aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs_level,\n\u001b[1;32m    406\u001b[0m         name_suffix\u001b[39m=\u001b[39;49mname_suffix,\n\u001b[1;32m    407\u001b[0m         infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    408\u001b[0m         infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m     model_names_fit \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m base_model_names \u001b[39m+\u001b[39m aux_models\n\u001b[1;32m    411\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_best \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(model_names_fit) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:539\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    537\u001b[0m     core_kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m core_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[1;32m    538\u001b[0m     aux_kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[0;32m--> 539\u001b[0m core_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_new_level_core(\n\u001b[1;32m    540\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    541\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    542\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    543\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    544\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    545\u001b[0m     models\u001b[39m=\u001b[39;49mmodels,\n\u001b[1;32m    546\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m    547\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    548\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    549\u001b[0m     base_model_names\u001b[39m=\u001b[39;49mbase_model_names,\n\u001b[1;32m    550\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcore_kwargs,\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    553\u001b[0m \u001b[39mif\u001b[39;00m X_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m     aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_new_level_aux(\n\u001b[1;32m    555\u001b[0m         X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my, base_model_names\u001b[39m=\u001b[39mcore_models, level\u001b[39m=\u001b[39mlevel \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, infer_limit\u001b[39m=\u001b[39minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39minfer_limit_batch_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39maux_kwargs\n\u001b[1;32m    556\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:673\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m fit_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[1;32m    672\u001b[0m \u001b[39m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi(\n\u001b[1;32m    674\u001b[0m     X\u001b[39m=\u001b[39;49mX_init,\n\u001b[1;32m    675\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    676\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    677\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    678\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    679\u001b[0m     models\u001b[39m=\u001b[39;49mmodels,\n\u001b[1;32m    680\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m    681\u001b[0m     stack_name\u001b[39m=\u001b[39;49mstack_name,\n\u001b[1;32m    682\u001b[0m     compute_score\u001b[39m=\u001b[39;49mcompute_score,\n\u001b[1;32m    683\u001b[0m     fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[1;32m    684\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    685\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2321\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[0;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[39mif\u001b[39;00m n_repeat_start \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2320\u001b[0m     time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m-> 2321\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_initial(\n\u001b[1;32m   2322\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m   2323\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m   2324\u001b[0m         models\u001b[39m=\u001b[39;49mmodels,\n\u001b[1;32m   2325\u001b[0m         k_fold\u001b[39m=\u001b[39;49mk_fold,\n\u001b[1;32m   2326\u001b[0m         n_repeats\u001b[39m=\u001b[39;49mn_repeats_initial,\n\u001b[1;32m   2327\u001b[0m         hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs,\n\u001b[1;32m   2328\u001b[0m         feature_prune_kwargs\u001b[39m=\u001b[39;49mfeature_prune_kwargs,\n\u001b[1;32m   2329\u001b[0m         time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[1;32m   2330\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2331\u001b[0m     )\n\u001b[1;32m   2332\u001b[0m     n_repeat_start \u001b[39m=\u001b[39m n_repeats_initial\n\u001b[1;32m   2333\u001b[0m     \u001b[39mif\u001b[39;00m time_limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2170\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[0;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2169\u001b[0m     time_ratio \u001b[39m=\u001b[39m hpo_time_ratio \u001b[39mif\u001b[39;00m hpo_enabled \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 2170\u001b[0m     models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_fold(\n\u001b[1;32m   2171\u001b[0m         models\u001b[39m=\u001b[39;49mmodels,\n\u001b[1;32m   2172\u001b[0m         hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs,\n\u001b[1;32m   2173\u001b[0m         k_fold_start\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2174\u001b[0m         k_fold_end\u001b[39m=\u001b[39;49mk_fold,\n\u001b[1;32m   2175\u001b[0m         n_repeats\u001b[39m=\u001b[39;49mn_repeats,\n\u001b[1;32m   2176\u001b[0m         n_repeat_start\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2177\u001b[0m         time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[1;32m   2178\u001b[0m         time_split\u001b[39m=\u001b[39;49mtime_split,\n\u001b[1;32m   2179\u001b[0m         time_ratio\u001b[39m=\u001b[39;49mtime_ratio,\n\u001b[1;32m   2180\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_args,\n\u001b[1;32m   2181\u001b[0m     )\n\u001b[1;32m   2183\u001b[0m multi_fold_time_elapsed \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m multi_fold_time_start\n\u001b[1;32m   2184\u001b[0m \u001b[39mif\u001b[39;00m time_limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2278\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[0;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2276\u001b[0m         time_start_model \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2277\u001b[0m         time_left \u001b[39m=\u001b[39m time_limit \u001b[39m-\u001b[39m (time_start_model \u001b[39m-\u001b[39m time_start)\n\u001b[0;32m-> 2278\u001b[0m model_name_trained_lst \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_single_full(\n\u001b[1;32m   2279\u001b[0m     X, y, model, time_limit\u001b[39m=\u001b[39;49mtime_left, hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs_model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   2280\u001b[0m )\n\u001b[1;32m   2282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m   2283\u001b[0m     \u001b[39mdel\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2051\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[0;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[1;32m   2047\u001b[0m         bagged_model_fit_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_bagged_model_fit_kwargs(\n\u001b[1;32m   2048\u001b[0m             k_fold\u001b[39m=\u001b[39mk_fold, k_fold_start\u001b[39m=\u001b[39mk_fold_start, k_fold_end\u001b[39m=\u001b[39mk_fold_end, n_repeats\u001b[39m=\u001b[39mn_repeats, n_repeat_start\u001b[39m=\u001b[39mn_repeat_start\n\u001b[1;32m   2049\u001b[0m         )\n\u001b[1;32m   2050\u001b[0m         model_fit_kwargs\u001b[39m.\u001b[39mupdate(bagged_model_fit_kwargs)\n\u001b[0;32m-> 2051\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_and_save(\n\u001b[1;32m   2052\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m   2053\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m   2054\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   2055\u001b[0m         X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m   2056\u001b[0m         y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m   2057\u001b[0m         X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m   2058\u001b[0m         stack_name\u001b[39m=\u001b[39;49mstack_name,\n\u001b[1;32m   2059\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   2060\u001b[0m         compute_score\u001b[39m=\u001b[39;49mcompute_score,\n\u001b[1;32m   2061\u001b[0m         total_resources\u001b[39m=\u001b[39;49mtotal_resources,\n\u001b[1;32m   2062\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs,\n\u001b[1;32m   2063\u001b[0m     )\n\u001b[1;32m   2064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[1;32m   2065\u001b[0m \u001b[39mreturn\u001b[39;00m model_names_trained\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1733\u001b[0m, in \u001b[0;36mAbstractTrainer._train_and_save\u001b[0;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1731\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs)\n\u001b[1;32m   1732\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1733\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_single(X, y, model, X_val, y_val, total_resources\u001b[39m=\u001b[39;49mtotal_resources, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs)\n\u001b[1;32m   1735\u001b[0m fit_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   1736\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_evaluation:\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1684\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single\u001b[0;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train_single\u001b[39m(\u001b[39mself\u001b[39m, X, y, model: AbstractModel, X_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, total_resources\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AbstractModel:\n\u001b[1;32m   1680\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \u001b[39m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m \u001b[39m    Returns trained model object.\u001b[39;00m\n\u001b[1;32m   1683\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1684\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, X_val\u001b[39m=\u001b[39;49mX_val, y_val\u001b[39m=\u001b[39;49my_val, total_resources\u001b[39m=\u001b[39;49mtotal_resources, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs)\n\u001b[1;32m   1685\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py:829\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_fit_resources(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    828\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_memory_usage(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 829\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    830\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py:169\u001b[0m, in \u001b[0;36mStackerEnsembleModel._fit\u001b[0;34m(self, X, y, compute_base_preds, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m time_limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     time_limit \u001b[39m=\u001b[39m time_limit \u001b[39m-\u001b[39m (time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time)\n\u001b[0;32m--> 169\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, time_limit\u001b[39m=\u001b[39;49mtime_limit, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py:266\u001b[0m, in \u001b[0;36mBaggedEnsembleModel._fit\u001b[0;34m(self, X, y, X_val, y_val, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, groups, _skip_oof, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[39m# Reserve time for final refit model\u001b[39;00m\n\u001b[1;32m    265\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m folds_to_fit \u001b[39m/\u001b[39m (folds_to_fit \u001b[39m+\u001b[39m \u001b[39m1.2\u001b[39m)\n\u001b[0;32m--> 266\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_folds(\n\u001b[1;32m    267\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    268\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    269\u001b[0m     model_base\u001b[39m=\u001b[39;49mmodel_base,\n\u001b[1;32m    270\u001b[0m     X_pseudo\u001b[39m=\u001b[39;49mX_pseudo,\n\u001b[1;32m    271\u001b[0m     y_pseudo\u001b[39m=\u001b[39;49my_pseudo,\n\u001b[1;32m    272\u001b[0m     k_fold\u001b[39m=\u001b[39;49mk_fold,\n\u001b[1;32m    273\u001b[0m     k_fold_start\u001b[39m=\u001b[39;49mk_fold_start,\n\u001b[1;32m    274\u001b[0m     k_fold_end\u001b[39m=\u001b[39;49mk_fold_end,\n\u001b[1;32m    275\u001b[0m     n_repeats\u001b[39m=\u001b[39;49mn_repeats,\n\u001b[1;32m    276\u001b[0m     n_repeat_start\u001b[39m=\u001b[39;49mn_repeat_start,\n\u001b[1;32m    277\u001b[0m     save_folds\u001b[39m=\u001b[39;49msave_bag_folds,\n\u001b[1;32m    278\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    279\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    280\u001b[0m )\n\u001b[1;32m    281\u001b[0m \u001b[39m# FIXME: Cleanup self\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39m# FIXME: Support `can_refit_full=False` models\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m refit_folds:\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py:592\u001b[0m, in \u001b[0;36mBaggedEnsembleModel._fit_folds\u001b[0;34m(self, X, y, model_base, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, time_limit, sample_weight, save_folds, groups, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[39mfor\u001b[39;00m fold_fit_args \u001b[39min\u001b[39;00m fold_fit_args_list:\n\u001b[1;32m    591\u001b[0m     fold_fitting_strategy\u001b[39m.\u001b[39mschedule_fold_model_fit(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfold_fit_args)\n\u001b[0;32m--> 592\u001b[0m fold_fitting_strategy\u001b[39m.\u001b[39;49mafter_all_folds_scheduled()\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# No need to add child times or save child here as this already occurred in the fold_fitting_strategy\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_child(model\u001b[39m=\u001b[39mmodel, add_child_times\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py:309\u001b[0m, in \u001b[0;36mSequentialLocalFoldFittingStrategy.after_all_folds_scheduled\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mafter_all_folds_scheduled\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    308\u001b[0m     \u001b[39mfor\u001b[39;00m job \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjobs:\n\u001b[0;32m--> 309\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_fold_model(job)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py:314\u001b[0m, in \u001b[0;36mSequentialLocalFoldFittingStrategy._fit_fold_model\u001b[0;34m(self, fold_ctx)\u001b[0m\n\u001b[1;32m    312\u001b[0m time_start_fold \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    313\u001b[0m time_limit_fold \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_fold_time_limit(fold_ctx)\n\u001b[0;32m--> 314\u001b[0m fold_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_base, time_start_fold, time_limit_fold, fold_ctx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_base_kwargs)\n\u001b[1;32m    315\u001b[0m fold_model, pred_proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_oof(fold_model, fold_ctx)\n\u001b[1;32m    316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_bagged_ensemble(fold_model, pred_proba, fold_ctx)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py:349\u001b[0m, in \u001b[0;36mSequentialLocalFoldFittingStrategy._fit\u001b[0;34m(self, model_base, time_start_fold, time_limit_fold, fold_ctx, kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m     num_cpus \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_cpus, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_resources_per_job\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnum_cpus\u001b[39m\u001b[39m\"\u001b[39m, math\u001b[39m.\u001b[39minf))\n\u001b[1;32m    348\u001b[0m     num_gpus \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_gpus, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_resources_per_job\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnum_gpus\u001b[39m\u001b[39m\"\u001b[39m, math\u001b[39m.\u001b[39minf))\n\u001b[0;32m--> 349\u001b[0m fold_model\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mX_fold, y\u001b[39m=\u001b[39;49my_fold, X_val\u001b[39m=\u001b[39;49mX_val_fold, y_val\u001b[39m=\u001b[39;49my_val_fold, time_limit\u001b[39m=\u001b[39;49mtime_limit_fold, num_cpus\u001b[39m=\u001b[39;49mnum_cpus, num_gpus\u001b[39m=\u001b[39;49mnum_gpus, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_fold)\n\u001b[1;32m    350\u001b[0m fold_model\u001b[39m.\u001b[39mfit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m time_start_fold\n\u001b[1;32m    351\u001b[0m \u001b[39mreturn\u001b[39;00m fold_model\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py:829\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_fit_resources(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    828\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_memory_usage(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 829\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    830\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py:211\u001b[0m, in \u001b[0;36mCatBoostModel._fit\u001b[0;34m(self, X, y, X_val, y_val, time_limit, num_gpus, num_cpus, sample_weight, sample_weight_val, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m eval_set \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     fit_final_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_best_model\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_final_kwargs)\n\u001b[1;32m    213\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams_trained[\u001b[39m\"\u001b[39m\u001b[39miterations\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtree_count_\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/catboost/core.py:5730\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5727\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m   5728\u001b[0m     CatBoostRegressor\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m-> 5730\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline,\n\u001b[1;32m   5731\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[1;32m   5732\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[1;32m   5733\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/catboost/core.py:2355\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2351\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2353\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[1;32m   2354\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2355\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[1;32m   2356\u001b[0m         train_pool,\n\u001b[1;32m   2357\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   2358\u001b[0m         params,\n\u001b[1;32m   2359\u001b[0m         allow_clear_pool,\n\u001b[1;32m   2360\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m   2361\u001b[0m     )\n\u001b[1;32m   2363\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.10/site-packages/catboost/core.py:1759\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1758\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1759\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   1760\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:4623\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4672\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=label, problem_type='regression', eval_metric=eval_metric\n",
    ").fit(train_data, presets=\"best_quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
